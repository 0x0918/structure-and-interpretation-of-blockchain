# 以太坊共识
区块链鼻祖比特币采用PoW共识，已经稳定运行10年。但在2011年开始，因为比特币有利可图，市场经济下出现了专业矿机（ASIC）专门针对哈希算法、散热、耗能进行优化，这脱离了比特币`one cpu on vote`的原则。这将造成节点中心化，面临51%攻击风险。因此以太坊需要预防风险和改进PoW。

在以太坊早期起草的共识算法是 Dagger-Hashimoto2， 但被证明 Dagger很容易受到 Sergio Lerner 共享内存硬件加速的影响。 所以最终抛弃了 Dagger-Hashimoto，改变研究方向。 在对 Dagger-Hashimoto 进行大量修改，终于形成了新算法：Ethash

## Dagger
Dagger算法由Vitalik Buterin发明，旨在通过DAG（有向无环图）来同时获得“memory-hard计算”和“memory-easy验证”这两个特性，其主要原理是针对每一个单独的nonce只需要访问数据集中的一小部分数据。
## Hashimoto
Hashimoto算法由Thaddeus Dryja发明，通过增加对内存读取瓶颈来抵制ASIC矿机。

## Dagger-Hashimoto
采用Dagger-Hashimoto算法是想要达到如下目标；
- 对抗ASIC专业矿机，避免算力集中化；
- 支持轻客户端，对硬件性能比较差的轻节点也能进行SPV验证；
- 全链存储数据；

Dagger-Hashimoto算法由Dagger和Hashimoto融合而成；

### Ethash
Ethash算法的总体思路是这样的，轻节点因为硬件设备和节点特性，存在计算能力不足，内存小的特点，而矿工因为挖矿需要计算大量哈希，甚至是专业设计的ASIC矿机，具有计算能力强，内存大的特点，为了对抗ASIC矿机就需要在挖矿时增加内存消耗，而验证时只需要很小的内存，避免了挖矿只需要算力的问题，使挖矿更贴近普通计算机，实践`one cpu one vote`。
> 这里可能会有疑问，既然是专业设计的矿机，为什么不可以针对Ethash算法设计计算能力强内存大的专业矿机？相较于提升计算能力，提升内存容量需要更高的成本和门槛，提升完内存之后，内存与cpu的带宽又会极大的限制内存的读取速度，而内存带宽又是很难提升的，这些限制导致设计针对Ethash算法的矿机困难重重。

Ethash算法流程；
- 根据区块信息生成一个种子（seed）;
- 根据seed计算出一个16M的伪随机cache，由轻客户端存储；
- 根据cache计算出一个1G的dataset，其中的每一个数据都是通过cache中的一小部分数据计算出来，该dataset由全节点存储，大小随时间线性增长；
- 矿工会从dataset中随机取出数据计算hash，来判断是否满足挖矿难度；
- 轻节点验证者会根据cache重新生成dataset中所需要的那部分数据，因此只需要存储cache即可；

> dataset又被称为DAG

![](https://github.com/Ice-Storm/structure-and-interpretation-of-blockchain/blob/master/img/chapter_7/6_1.png?raw=true)


具体实现；

- 在以太坊中，每30000个区块称作一个`纪元（epoch）`，每产生一个纪元就更新一次dataset和cache。

- 前2048个纪元的生成的dataset和cache的大小是硬编码在代码里的，如果超过这个数量就需要自己计算了。

    1. dataset的计算方式(2^24 + 2^17 * epoch - 128)，用这个值除以128看结果是否是一个质数，如果不是，减去128再重新计算，直到找到最大的质数为止。

    2. cache的计算方式(2^24 + 2^17 * epoch - 64)，用这个值除以64看结果是否是一个质数，如果不是，减去64再重新计算，直到找到最大的质数为止。

> dataset从1GB 开始，以每年约 520MB 的速度增大，cache从16MB 开始，以每年约 12MB 的速度增大。

Ethash是一种“memory-hard计算”和“memory-easy验证”的哈希算法，通过内存访问速度的瓶颈抵抗ASIC矿机，同时利用两级数据集实现挖矿和轻客户端验证的分离。

在介绍以太坊具体实现的时候，先介绍一下以太坊挖矿用到的FNV哈希算法。

### FNV哈希算法
FNV哈希算法全名为Fowler-Noll-Vo，是以三位发明人Glenn Fowler，Landon Curt Noll，Phong Vo的名字来命名的，最早在1991年提出。
FNV算法有三个版本：FNV-0（已废弃）、FNV-1和FNV-1a，两个算法需要用到的变量如下；
```
hash值：一个n位的unsigned int型hash值
offset_basis：初始的哈希值
FNV_prime：FNV用于散列的质数
octet_of_data：8位数据（即一个字节）
```

FNV-1和FNV-1a的算法都很简单，FNV-1算法如下
```
hash = offset_basis
for each octet_of_data to be hashed
    hash = hash * FNV_prime
    hash = hash xor octet_of_data
return hash
```

FNV-1a算法如下；
```
hash = offset_basis 
for each octet_of_data to be hashed
    hash = hash xor octet_of_data
    hash = hash * FNV_prime
return hash
```
FNV_prime的取值: 
32 bit FNV_prime = 2^24 + 2^8 + 0x93 = 16777619
64 bit FNV_prime = 2^40 + 2^8 + 0xb3 = 1099511628211

> 以太坊采用的是FNV-1


### 以太坊实现

![](https://github.com/Ice-Storm/structure-and-interpretation-of-blockchain/blob/master/img/chapter_7/6_2.jpg?raw=true)

#### 生成种子（Seed）
首先将header和nonce合并成一个40Bytes长的数组，取它的SHA-512哈希值，作为seed，长度为32Bytes。
```
func seedHash(block uint64) []byte {
	seed := make([]byte, 32)
	if block < epochLength {
		return seed
	}
	keccak256 := makeHasher(sha3.NewLegacyKeccak256())
	for i := 0; i < int(block/epochLength); i++ {
		keccak256(seed, seed)
	}
	return seed
}
```

#### 计算缓存大小
根据区块高度，可计算验证区块所需要的缓存大小，为了加速计算速度，已经根据增长算法，内置了最早 1024 个时期的缓存大小。相当于在 30720000 区块前是可以直接使用缓存的。 否则根据区块所在时期，进行缓存大小计算。

```
// 如果是之前是计算好的值则直接返回，否则进行计算
func cacheSize(block uint64) uint64 {
    epoch := int(block / epochLength)
    if epoch < maxEpoch {
        return cacheSizes[epoch]
    }
    return calcCacheSize(epoch)
}

func calcCacheSize(epoch int) uint64 {
    size := cacheInitBytes +
            cacheGrowthBytes*uint64(epoch) - hashBytes// 3

    for !new(big.Int).SetUint64(size / hashBytes).ProbablyPrime(1) { // 4
        size -= 2 * hashBytes
    }
    return size
}
```

根据基础量和增长量计算出缓存大小3，但不能直接使用这个阀值，而是采用低于阀值的一个素数4，是为了帮助降低循环行为的偶然规律性的风险。

同样也提供了数据集的计算方式。
```
// consensus/ethash/algorithm.go:74
func datasetSize(block uint64) uint64 {
    epoch := int(block / epochLength)
    if epoch < maxEpoch {
		return datasetSizes[epoch]
	}
	return calcDatasetSize(epoch)
}

func calcDatasetSize(epoch int) uint64 {
	size := datasetInitBytes + datasetGrowthBytes*uint64(epoch) - mixBytes
	for !new(big.Int).SetUint64(size / mixBytes).ProbablyPrime(1) {
		size -= 2 * mixBytes
	}
	return size
}
```

#### 生成缓存
根据已计算出的缓存大小、区块周期、种子，则可以生成伪随机填充的缓存数据，代码实现在 generateCache 函数中。

```
// consensus/ethash/algorithm.go:139
func generateCache(dest []uint32, epoch uint64, seed []byte) {
    // ...
	header := *(*reflect.SliceHeader)(unsafe.Pointer(&dest))//❶
	header.Len *= 4
	header.Cap *= 4
	cache := *(*[]byte)(unsafe.Pointer(&header))

	// ...
	size := uint64(len(cache))
	rows := int(size) / hashBytes
    // ...
	keccak512 := makeHasher(sha3.NewLegacyKeccak512())
	keccak512(cache, seed)//❷
	for offset := uint64(hashBytes); offset < size; offset += hashBytes {//❸
		keccak512(cache[offset:], cache[offset-hashBytes:offset])
		atomic.AddUint32(&progress, 1)
	}
	temp := make([]byte, hashBytes)

	for i := 0; i < cacheRounds; i++ {//❹
		for j := 0; j < rows; j++ {
			var (
				srcOff = ((j - 1 + rows) % rows) * hashBytes
				dstOff = j * hashBytes
				xorOff = (binary.LittleEndian.Uint32(cache[dstOff:]) % uint32(rows)) * hashBytes
			)
			bitutil.XORBytes(temp, cache[srcOff:srcOff+hashBytes], cache[xorOff:xorOff+hashBytes])
			keccak512(cache[dstOff:], temp)

			atomic.AddUint32(&progress, 1)
		}
	}
	if !isLittleEndian() {//❺
		swap(cache)
	}
}
```
缓存生产过程，首先将切片大小放大4倍（调用前外部有缩小四倍），以便在内存中填充出一块内存❶。 比如第一个周期内是填充出 16MB 内存。然后，将种子的哈希作为初始化值写入缓存的前64字节中❷， 随后，以一个哈希值长度(64字节)为单位，依次进行哈希，将内容填充到缓存中，完成缓存内容的顺序填充。

![](https://github.com/Ice-Storm/structure-and-interpretation-of-blockchain/blob/master/img/chapter_7/6_10.jpg?raw=true)

因为所有的计算都是不断进行哈希运算，因此不管在前面确认缓存大小，还是在缓存生成时都是以一个哈希值作为最新计算值。 在计算缓存大小时，已经是根据哈希值长度进行缓存大小计算，所以缓存能被 64 整除，且整除值为一个素数。

接下来，执行了3次（cacheRounds）在严格内存硬哈希函数 Strict Memory Hard Hashing Functions (2014)1 中定义的内存难题算法 RandMemoHash ❹，算法描述请参考论文。该生成算法的目的是为了证明这一刻确实使用了指定量的内存进行计算。

最后，如果操作系统是 Big-Endian(非little-endian)的字节序，那么意味着低位字节排放在内存的高端，高位字节排放在内存的低端。此时，将缓存内容进行倒排，以便调整内存存放顺序。最终使得，缓存内容在内存中排序顺序和机器字节顺序一致

#### 生成数据集
有了缓存，便可以来生成数据集。生成过程中不断重复从缓存中合成64字节的数据依次填充到数据中。

![](https://github.com/Ice-Storm/structure-and-interpretation-of-blockchain/blob/master/img/chapter_7/6_11.jpg?raw=true)

1GB的数据集，则需要填充 16777216 次，每次都是根据索引 index 从缓存中获取 64 字节数据作为初始值，并进行依次哈希计算。

随后，哈希后的 64 字节数据还需要执行 256 次 fnvHash 计算。 最终对计算结果进行哈希，得到最终需要的 64 位字节。 并填充到数据集中。

这样，1 GB数据需要进行 16777216 * 256 次计算。因此此过程还是非常耗时的，因此在 geth 中是重复利用多核进行并行计算。即使如此，1 GB数据集运行也是缓慢的。

这也就是为什么在搭建私有链时，刚开始时会看到一段“Generating DAG in progress” 的日志，直到生成数据集完成后，才可以开始挖矿。 可以执行 geth 的子命令dgeth makedag 10000 /tmp/ethdag来直接生成数据集。

![](https://github.com/Ice-Storm/structure-and-interpretation-of-blockchain/blob/master/img/chapter_6/6_12.jpg?raw=true)

### 挖矿

挖矿所需具备的数据集准备好后，则在每次需要挖出新块时，需要结合新区块链信息、数据集、随机数来进行数据聚合计算， 如果此算法的输出 result 低于目标 target，则nonce有效。

#### 生成混淆数据
将seed转换成uint32类型的数组mix[]uint32，一个uint32数等于4Bytes，mix数组的长度为32，通过如下算法填充mix；

```

mix := make([]uint32, mixBytes/4)

for i := 0; i < len(mix); i++ {

  mix[i] = binary.LittleEndian.Uint32(seed[i%16*4:])

}

将seed转换为小端序放入mix
```

接着通过FNV算法混淆mix数组中的每一个元素；

```

 for i := 0; i < loopAccesses; i++ {

    parent := fnv(uint32(i)^seedHead, mix[i%len(mix)]) % rows

    for j := uint32(0); j < mixBytes/hashBytes; j++ {

        copy(temp[j*hashWords:], lookup(2*parent+j))

    }

    fnvHash(mix, temp)

}

```

- 混淆完成后通过FNV算法将mix折叠到原来的1/4；

```

for i := 0; i < len(mix); i += 4 {

    mix[i/4] = fnv(fnv(fnv(mix[i], mix[i+1]), mix[i+2]), mix[i+3])

}

mix = mix[:len(mix)/4]

```

最后，将折叠后的 mix[]uint32 由长度为8的uint32 型数组直接转化成一个长度32的 byte 数组，这就是返回值 digest；同时将之前的 seed[] 数组与 digest 合并再取一次 SHA-256 哈希值，得到的长度32的 byte 数组，即返回值 result。

```

digest := make([]byte, common.HashLength)

 for i, val := range mix {

  binary.LittleEndian.PutUint32(digest[i*4:], val)

 }

 return digest, crypto.Keccak256(append(seed, digest...))

```

hashimoto 返回两个长度均为32的 byte 数组 digest 和 result，前文已提到，在 Ethash 的 mine 方法里，挖矿时需要经过一个死循环，直到找到一个 nonce，使得 hashimoto 返回的 result 和 target 是相等的，这时就表示符合要求。


其中的数据流如下图所示；

![](https://github.com/Ice-Storm/structure-and-interpretation-of-blockchain/blob/master/img/chapter_7/6_3.png?raw=true)


### 难度炸弹

以太坊“难度炸弹”是以太坊在2015年加入的一段代码，通过逐步增加区块链挖矿难度，从而人为减慢以太坊发行速度。这一机制是为了使以太坊的共识算法从工作量证明向权益证明机制算法的巨大转变而设计的。

难度炸弹指的是计算难度时除了根据出块时间和上一个区块难度进行调整外，加上了一个每十万个区块呈指数型增长的难度因子。

这有点像一个温水煮青蛙的过程，一开始附加的难度并不引人注意，但是随着区块高度的增加，呈指数增长的难度因子比重将会显著提高，使得出块难度大大增加，矿工将难以挖出新的区块。在2017年9月的时候以太坊的区块高度超过420万，难度炸弹已经开始发挥威力，出块时间从之前很长一段时间维持的平均15秒左右渐渐增加到了25秒，每天新产生的ETH降到了19000以下（2017年9月2日数据）。由于出块越来越艰难，到最后区块将被完全冻结，这个过程又被称作“冰川时代”（Ice Age）。有了这个预期，那么转PoS引起的硬分叉就不会是一个困难的选择，毕竟没有人会继续待在那条将要走向凛冬的区块链。

可以通过下图看到难度炸弹的威力:

![](https://github.com/Ice-Storm/structure-and-interpretation-of-blockchain/blob/master/img/chapter_7/6_9.png?raw=true)


然而PoS的机制设计中有很多问题需要解决，开发时间比原本计划的要长。根据最近的以太坊改进建议EIP-649（2017年8月26日被接受 ）, 转换到权益证明（PoS）的时间节点将被延迟约一年半，工作量证明（PoW）将会继续担当大任。为了不堵塞交易，维持系统稳定运行，难度炸弹也需要被相应地延迟，实现方式是将挖矿难度按照回退300万个区块的高度去计算，因此出块时间又将回到15秒左右，如果不采取任何行动，则ETH的供应量会明显超出按原本难度炸弹时间表规划的供应量，这会导致通货膨胀，降低ETH的价值，为了使ETH的供应量与原本计划的数量相当，于是需要减少每个区块的奖励，从原本的5个ETH减少为3个ETH，叔块的奖励也将相应减少。

在目前的工作量证明共识机制的条件下，矿工每次创建新区块并将其添加到区块链中时都会赢得奖励。但当以太坊难度炸弹设置为“引爆”时，矿工通过挖矿获得奖励的难度将成倍增加。
### 以太坊挖矿难度调整

与比特币每2016个区块调整挖矿难度不同，在以太坊中每个区块都有可能调整挖矿难度，调整的公式如下；

本区块难度 = 父区块难度 + 难度调整 + 难度炸弹

难度调整 = 父区块难度 / 2048 * MAX(1 - (block_timestamp - parent_timestamp) / 10, -99)

难度炸弹 = INT(2**((block_number / 100000) - 2))

轻节点验证
```

func hashimoto(hash []byte, nonce uint64, size uint64, lookup func(index uint32) []uint32) ([]byte, []byte) {

 // Calculate the number of theoretical rows (we use one buffer nonetheless)

 rows := uint32(size / mixBytes)


 // Combine header+nonce into a 64 byte seed

 seed := make([]byte, 40)

 copy(seed, hash)

 binary.LittleEndian.PutUint64(seed[32:], nonce)



 seed = crypto.Keccak512(seed)

 seedHead := binary.LittleEndian.Uint32(seed)



 // Start the mix with replicated seed

 mix := make([]uint32, mixBytes/4)

 for i := 0; i < len(mix); i++ {

  mix[i] = binary.LittleEndian.Uint32(seed[i%16*4:])

 }

 // Mix in random dataset nodes

 temp := make([]uint32, len(mix))



 for i := 0; i < loopAccesses; i++ {

  parent := fnv(uint32(i)^seedHead, mix[i%len(mix)]) % rows

  for j := uint32(0); j < mixBytes/hashBytes; j++ {

   copy(temp[j*hashWords:], lookup(2*parent+j))

  }

  fnvHash(mix, temp)

 }

 // Compress mix

 for i := 0; i < len(mix); i += 4 {

  mix[i/4] = fnv(fnv(fnv(mix[i], mix[i+1]), mix[i+2]), mix[i+3])

 }

 mix = mix[:len(mix)/4]



 digest := make([]byte, common.HashLength)

 for i, val := range mix {

  binary.LittleEndian.PutUint32(digest[i*4:], val)

 }

 return digest, crypto.Keccak256(append(seed, digest...))

}

```
